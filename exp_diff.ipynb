{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "2. [Methods](#methods)\n",
    "    1. [Original tangent approximation](#org-tangent)\n",
    "    2. [Adapted tangent approximation](#adapted-tangent)\n",
    "    3. [Finite difference approximation](#finite-diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Differentiation of matrix exp for weighted GNMs <a name=\"intro\"></a>\n",
    "From: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "This is mostly concerned with finding the fastest way to compute the derivative of the matrix exponential.\n",
    "$$\n",
    "f(w_{i,j}) = (c_{i,k} * d_{i,j})^{\\omega}\n",
    "$$\n",
    "Where, the communicability matrix is defined as follows. This captures the proportion of signals, that propagate randomly from node i would reach node j over an infinite time horizon.\n",
    "$$\n",
    "c_{i,j} = e^{s^{-1/2}w_{i,j}s^{-1/2}}\n",
    "$$\n",
    "\n",
    "In this demonstration, we will simply work on the matrix exponential differentiation:\n",
    "$$\n",
    "f(W)=e^{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.0  0.8  0.0\n",
       " 0.8  0.0  0.2\n",
       " 0.0  0.2  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "demo_edge = [CartesianIndex(1, 2)]\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods <a name=\"methods\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original tangent approximation <a name=\"org-tangent\"></a>\n",
    "\n",
    "Here we are computing the derivative for a single edge. We will use the tangent approximation to compute the derivative. \n",
    "This is the approach used in the Akarca paper.\n",
    "\n",
    "**Problem 1**\n",
    "As far as I understand, there is a small but significant error in the implementation provided at https://github.com/DanAkarca/weighted_generative_models/blob/main/weighted_generative_model.m in line 197.\n",
    "When fitting the first-order polynomial, the independent variable should not be the range of the differences to the edge, which have been used, but should be the actual values, the edge is taking on.\n",
    "\n",
    "**Problem 2**\n",
    "In the same implementation the derivative is approximated from the tangent approximation after nudging the edge (i,j) as well as (j,i).\n",
    "In a sense we are computing the derivative with respect to both (i,j) and (j,i) at once.\n",
    "First of all, this will create computational overhead, as the derivatives (i.e., the jacobian) will also always be symmetric.\n",
    "Secondly, this will create a problem when we are trying to compute the gradient, which we currently simply take as the the derivative.\n",
    "But then we update both elements in the W matrix with the same gradient which is the derivative with respect to nudging both (i,j) and (j,i) - thus our update to the objective function is overshooting.\n",
    "\n",
    "See line 208 and 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.038767237882607476], [0.7456781939183535])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, resolution = 0.01, min = -0.25, max = 0.25)\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(min:resolution:max)\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted tangent approximation <a name=\"adapted-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.8459047353259335], [4.851647952441173])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, min = -0.25, max = 0.25)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(min:resolution:max)\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(reps, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "tangent_approx(W, demo_edge, 0.01), tangent_approx(W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference method <a name=\"finite-diff\"></a>\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "At the moment, this is a significantly faster but less accurate method than the tangent approximation.\n",
    "Only implemented to be potentially extended to complex step approximation.\n",
    "\n",
    "References:<br>\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.826503769214696], [4.827101432180703])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function finite_diff(W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "\n",
    "finite_diff(W, demo_edge, 0.01)*2, finite_diff(W, demo_edge,0.2)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.8612290825962985"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jₓ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = dot(Jₓ, tangent)\n",
    "    end\n",
    "\n",
    "    return results .* 2\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.861229082596298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    tangent = exp(W)\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges] .* 2\n",
    "end\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Block enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.861229082596298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function frechet_block_enlarge(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = exp(W)\n",
    "    n = size(W, 1)\n",
    "    M = [W E; zeros(size(W)) W]\n",
    "    expm_M = exp(M) \n",
    "    frechet_AE = expm_M[1:n, n+1:end]\n",
    "    return frechet_AE[edges] .* 2\n",
    "end\n",
    "\n",
    "frechet_block_enlarge(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Scaling-Pade-Squaring\n",
    "\n",
    "\n",
    "References:<br>\n",
    "Higham (2008), Functions of Matrices - Theory and Computation\", Chapter 10, Algorithm 10.20 <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet <br>\n",
    "https://rdrr.io/cran/expm/man/expmFrechet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nothing, 2.11e-8, 0.000356, 0.0108, 0.0649, 0.2, 0.437, 0.783, 1.23, 1.78, 2.42, 3.13, 3.9, 4.74, 5.63, 6.56, 7.52, 8.53, 9.56, 10.6, 11.7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function _diff_pade3(A, E, ident)\n",
    "    b = (120.0, 60.0, 12.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    U = A * (b[4] * A2 + b[2] * ident)\n",
    "    V = b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[4] * M2) + E * (b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "    \n",
    "function _diff_pade5(A, E, ident)\n",
    "    b = (30240.0, 15120.0, 3360.0, 420.0, 30.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    U = A * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[6] * M4 + b[4] * M2) + E * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade7(A, E, ident)\n",
    "    b = (17297280.0, 8648640.0, 1995840.0, 277200.0, 25200.0, 1512.0, 56.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    U = A * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade9(A, E, ident)\n",
    "    b = (17643225600.0, 8821612800.0, 2075673600.0, 302702400.0, 30270240.0, 2162160.0, 110880.0, 3960.0, 90.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    A8 = A4 * A4\n",
    "    M8 = A4 * M4 + M4 * A4\n",
    "    U = A * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[9] * A8 + b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[10] * M8 + b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[9] * M8 + b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "ell_table_61 = (\n",
    "        nothing,\n",
    "        2.11e-8,\n",
    "        3.56e-4,\n",
    "        1.08e-2,\n",
    "        6.49e-2,\n",
    "        2.00e-1,\n",
    "        4.37e-1,\n",
    "        7.83e-1,\n",
    "        1.23e0,\n",
    "        1.78e0,\n",
    "        2.42e0,\n",
    "        # 11\n",
    "        3.13e0,\n",
    "        3.90e0,\n",
    "        4.74e0,\n",
    "        5.63e0,\n",
    "        6.56e0,\n",
    "        7.52e0,\n",
    "        8.53e0,\n",
    "        9.56e0,\n",
    "        1.06e1,\n",
    "        1.17e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.861229082596298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function frechet_algo(A::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = exp(A)\n",
    "    n = size(A, 1)\n",
    "    s = nothing\n",
    "    ident = Matrix{Float64}(I, n, n)\n",
    "    A_norm_1 = norm(A, 1)\n",
    "    m_pade_pairs = [\n",
    "        (3, _diff_pade3),\n",
    "        (5, _diff_pade5),\n",
    "        (7, _diff_pade7),\n",
    "        (9, _diff_pade9)\n",
    "    ]\n",
    "    for (m, pade) in m_pade_pairs\n",
    "        if A_norm_1 <= ell_table_61[m]\n",
    "            U, V, Lu, Lv = pade(A, E, ident)\n",
    "            s = 0\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if s == nothing\n",
    "        # scaling\n",
    "        s = max(0, ceil(Int, log2(A_norm_1 / ell_table_61[13])))\n",
    "        A *= 2.0^-s\n",
    "        E *= 2.0^-s\n",
    "        # pade order 13\n",
    "        A2 = A * A\n",
    "        M2 = A * E + E * A\n",
    "        A4 = A2 * A2\n",
    "        M4 = A2 * M2 + M2 * A2\n",
    "        A6 = A2 * A4\n",
    "        M6 = A4 * M2 + M4 * A2\n",
    "        b = (64764752532480000., 32382376266240000., 7771770303897600.,\n",
    "                1187353796428800., 129060195264000., 10559470521600.,\n",
    "                670442572800., 33522128640., 1323241920., 40840800., 960960.,\n",
    "                16380., 182., 1.)\n",
    "        W1 = b[14] * A6 + b[12] * A4 + b[10] * A2\n",
    "        W2 = b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident\n",
    "        Z1 = b[13] * A6 + b[11] * A4 + b[9] * A2\n",
    "        Z2 = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "        W = A6 * W1 + W2\n",
    "        U = A * W\n",
    "        V = A6 * Z1 + Z2\n",
    "        Lw1 = b[14] * M6 + b[12] * M4 + b[10] * M2\n",
    "        Lw2 = b[8] * M6 + b[6] * M4 + b[4] * M2\n",
    "        Lz1 = b[13] * M6 + b[11] * M4 + b[9] * M2\n",
    "        Lz2 = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "        Lw = A6 * Lw1 + M6 * W1 + Lw2\n",
    "        Lu = A * Lw + E * W\n",
    "        Lv = A6 * Lz1 + M6 * Z1 + Lz2\n",
    "    end\n",
    "    # factor once and solve twice\n",
    "    lu_piv = lu(-U + V)\n",
    "    R = lu_piv \\ (U + V)\n",
    "    L = lu_piv \\ (Lu + Lv + ((Lu - Lv) * R))\n",
    "    # squaring\n",
    "    for k in 1:s\n",
    "        L = R * L + L * R\n",
    "    end\n",
    "    return L[edges] .*2\n",
    "end\n",
    "\n",
    "frechet_algo(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking <a name=\"benchmarking\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.605 s (264026 allocations: 1.73 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84.730 ms (7317 allocations: 57.23 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.717 s (5867 allocations: 311.91 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.711 ms (39 allocations: 432.30 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.319 ms (43 allocations: 726.62 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  575.458 μs (221 allocations: 1.80 MiB)\n"
     ]
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "\n",
    "@btime tangent_approx($W_bench, $edges_bench, 0.01) \n",
    "@btime finite_diff($W_bench, $edges_bench, 0.1) \n",
    "@btime forward_diff_j($W_bench, $edges_bench) \n",
    "@btime forward_diff_jvp($W_bench, $edges_bench) \n",
    "@btime frechet_block_enlarge($W_bench, $edges_bench) \n",
    "@btime frechet_algo($W_bench, $edges_bench) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension to full objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
