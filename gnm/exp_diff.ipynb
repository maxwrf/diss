{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "2. [Methods](#methods)\n",
    "    1. [Original tangent approximation](#org-tangent)\n",
    "    2. [Adapted tangent approximation](#adapted-tangent)\n",
    "    3. [Finite difference approximation](#finite-diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Differentiation of matrix exp for weighted GNMs <a name=\"intro\"></a>\n",
    "From: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "This is mostly concerned with finding the fastest way to compute the derivative of the matrix exponential.\n",
    "$$\n",
    "f(w_{i,j}) = (c_{i,k} * d_{i,j})^{\\omega}\n",
    "$$\n",
    "Where, the communicability matrix is defined as follows. This captures the proportion of signals, that propagate randomly from node i would reach node j over an infinite time horizon.\n",
    "$$\n",
    "c_{i,j} = e^{s^{-1/2}w_{i,j}s^{-1/2}}\n",
    "$$\n",
    "\n",
    "In this demonstration, we will simply work on the matrix exponential differentiation:\n",
    "$$\n",
    "f(W)=e^{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_weight_test_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "using StatsBase: sample\n",
    "include(\"test_data.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.0  0.8  0.0\n",
       " 0.8  0.0  0.2\n",
       " 0.0  0.2  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "demo_edge = [CartesianIndex(1, 2)]\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods <a name=\"methods\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original tangent approximation <a name=\"org-tangent\"></a>\n",
    "\n",
    "Here we are computing the derivative for a single edge. We will use the tangent approximation to compute the derivative. \n",
    "This is the approach used in the Akarca paper.\n",
    "\n",
    "**Problem 1**\n",
    "As far as I understand, there is a small but significant error in the implementation provided at https://github.com/DanAkarca/weighted_generative_models/blob/main/weighted_generative_model.m in line 197.\n",
    "When fitting the first-order polynomial, the independent variable should not be the range of the differences to the edge, which have been used, but should be the actual values, the edge is taking on.\n",
    "\n",
    "**Problem 2**\n",
    "In the same implementation the derivative is approximated from the tangent approximation after nudging the edge (i,j) as well as (j,i).\n",
    "In a sense we are computing the derivative with respect to both (i,j) and (j,i) at once.\n",
    "First of all, this will create computational overhead, as the derivatives (i.e., the jacobian) will also always be symmetric.\n",
    "Secondly, this will create a problem when we are trying to compute the gradient, which we currently simply take as the the derivative.\n",
    "But then we update both elements in the W matrix with the same gradient which is the derivative with respect to nudging both (i,j) and (j,i) - thus our update to the objective function is overshooting.\n",
    "\n",
    "See line 208 and 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03861909629029788], [0.830380354061412])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted tangent approximation <a name=\"adapted-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.4132596676512077], [2.4141043556312676])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tangent_approx(f, W::Matrix{Float64}, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = rep \n",
    "            sum_comm[i_rep] = f(W_copy, edge_idx)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(reps, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = (W, _) -> sum(exp(W))\n",
    "tangent_approx(f, W, demo_edge, 0.01), tangent_approx(f, W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference method <a name=\"finite-diff\"></a>\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "At the moment, this is a significantly faster but less accurate method than the tangent approximation.\n",
    "Only implemented to be potentially extended to complex step approximation.\n",
    "\n",
    "References:<br>\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.413251884607348], [2.4135507160903513])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function finite_diff(f, W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx] = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = W -> sum(exp(W))\n",
    "finite_diff(f, W, demo_edge, 0.01), finite_diff(f, W, demo_edge,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.4132511356618336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jₓ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = sum(Jₓ)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    tangent = ones(size(W))\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges]\n",
    "end\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Block enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function frechet_block_enlarge(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(W))\n",
    "    n = size(W, 1)\n",
    "    M = [W E; zeros(size(W)) W]\n",
    "    expm_M = exp(M) \n",
    "    frechet_AE = expm_M[1:n, n+1:end]\n",
    "    return frechet_AE[edges]\n",
    "end\n",
    "\n",
    "frechet_block_enlarge(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Scaling-Pade-Squaring\n",
    "\n",
    "The algorithm:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{for } m = [3, 5, 7, 9] \\\\\n",
    "\\quad \\quad \\text{if } ||A||_{1} \\leq \\theta_{m} \\\\\n",
    "\\quad \\quad \\quad \\quad r_{m}(A) \\quad \\text{ \\textit{form Pade approximant to A}} \\\\\n",
    "\\quad \\quad \\quad \\quad U, V     \\quad \\text{ \\textit{Evaluate U and V, see below for }} r_{13} \\\\\n",
    "\\quad \\quad \\quad \\quad s=0 \\\\\n",
    "\\quad \\quad \\quad \\quad   \\text{break} \\\\\n",
    "\\quad \\quad \\text{end} \\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{if } ||A||_{1} \\geq  \\theta_{m[-1]} \\\\\n",
    "\\quad \\quad s = \\text{ceil}(\\log_{2}(||A||_{1}/ \\theta_{13})) \\\\\n",
    "\\quad \\quad A = A/2^{s} \\\\\n",
    "\\quad \\quad A_{2} = A^{2}, A_{4} = A_{2}^{2}, A_{6} = A_{2}A_{4}\\\\ \n",
    "\\quad \\quad U = A[A_{6}(b_{13} A_{6} + b_{11} A_{4} + b_{9} A_{2}) + b_{7} A_{6} + b_{5} A_{4} + b_{3} A_{2} + b_{1} I]\\\\\n",
    "\\quad \\quad V = A_{6}(b_{12}A_{6} + b_{10}A_{4} + b_{8}A_{2}) + b_{6}A_{6} + b_{4}A_{4} + b_{2}A_{2} + b_{0}I\\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{Solve } (-U+V)r_{m}(A) = U + V \\text{ for } r_{m} \\\\\n",
    "\\text{for } k = 1:s \\\\\n",
    "\\quad \\quad r_{m} = r_{m}*r_{m} \\\\\n",
    "\\text{end} \\\\\n",
    "\\text{return } r_{m} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "References:<br>\n",
    "Higham (2008), Functions of Matrices - Theory and Computation\", Chapter 10, Algorithm 10.20 <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet <br>\n",
    "https://rdrr.io/cran/expm/man/expmFrechet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function _diff_pade3(A, E, ident)\n",
    "    b = (120.0, 60.0, 12.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    U = A * (b[4] * A2 + b[2] * ident)\n",
    "    V = b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[4] * M2) + E * (b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "    \n",
    "function _diff_pade5(A, E, ident)\n",
    "    b = (30240.0, 15120.0, 3360.0, 420.0, 30.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    U = A * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[6] * M4 + b[4] * M2) + E * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade7(A, E, ident)\n",
    "    b = (17297280.0, 8648640.0, 1995840.0, 277200.0, 25200.0, 1512.0, 56.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    U = A * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade9(A, E, ident)\n",
    "    b = (17643225600.0, 8821612800.0, 2075673600.0, 302702400.0, 30270240.0, \n",
    "         2162160.0, 110880.0, 3960.0, 90.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    A8 = A4 * A4\n",
    "    M8 = A4 * M4 + M4 * A4\n",
    "    U = A * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[9] * A8 + b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[10] * M8 + b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[9] * M8 + b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade13(A, E, ident)\n",
    "    # pade order 13\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    b = (64764752532480000., 32382376266240000., 7771770303897600.,\n",
    "            1187353796428800., 129060195264000., 10559470521600.,\n",
    "            670442572800., 33522128640., 1323241920., 40840800., 960960.,\n",
    "            16380., 182., 1.)\n",
    "    W1 = b[14] * A6 + b[12] * A4 + b[10] * A2\n",
    "    W2 = b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident\n",
    "    Z1 = b[13] * A6 + b[11] * A4 + b[9] * A2\n",
    "    Z2 = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    W = A6 * W1 + W2\n",
    "    U = A * W\n",
    "    V = A6 * Z1 + Z2\n",
    "    Lw1 = b[14] * M6 + b[12] * M4 + b[10] * M2\n",
    "    Lw2 = b[8] * M6 + b[6] * M4 + b[4] * M2\n",
    "    Lz1 = b[13] * M6 + b[11] * M4 + b[9] * M2\n",
    "    Lz2 = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    Lw = A6 * Lw1 + M6 * W1 + Lw2\n",
    "    Lu = A * Lw + E * W\n",
    "    Lv = A6 * Lz1 + M6 * Z1 + Lz2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "ell_table_61 = (nothing, 2.11e-8, 3.56e-4, 1.08e-2, 6.49e-2, 2.00e-1, 4.37e-1,\n",
    "        7.83e-1, 1.23e0, 1.78e0, 2.42e0, 3.13e0, 3.90e0, 4.74e0, 5.63e0,\n",
    "        6.56e0, 7.52e0, 8.53e0, 9.56e0, 1.06e1, 1.17e1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function frechet_algo(A::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(A))\n",
    "    n = size(A, 1)\n",
    "    s = nothing\n",
    "    ident = Matrix{Float64}(I, n, n)\n",
    "    A_norm_1 = norm(A, 1)\n",
    "    m_pade_pairs = [(3, _diff_pade3),(5, _diff_pade5),(7, _diff_pade7),(9, _diff_pade9)]\n",
    "    for (m, pade) in m_pade_pairs\n",
    "        if A_norm_1 <= ell_table_61[m]\n",
    "            U, V, Lu, Lv = pade(A, E, ident)\n",
    "            s = 0\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if s == nothing\n",
    "        # scaling\n",
    "        s = max(0, ceil(Int, log2(A_norm_1 / ell_table_61[13])))\n",
    "        A *= 2.0^-s\n",
    "        E *= 2.0^-s\n",
    "        U, V, Lu, Lv = _diff_pade13(A, E, ident)\n",
    "    end\n",
    "    \n",
    "    # factor once and solve twice\n",
    "    lu_piv = lu(-U + V)\n",
    "    R = lu_piv \\ (U + V)\n",
    "    L = lu_piv \\ (Lu + Lv + ((Lu - Lv) * R))\n",
    "    \n",
    "    # repeated squaring\n",
    "    for k in 1:s\n",
    "        L = R * L + L * R\n",
    "        R= R * R\n",
    "    end\n",
    "    return L[edges]\n",
    "end\n",
    "\n",
    "frechet_algo(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking <a name=\"benchmarking\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.584 s (240215 allocations: 1.80 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  97.893 ms (8469 allocations: 66.25 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.732 s (6067 allocations: 312.60 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.503 ms (20 allocations: 315.42 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.672 ms (24 allocations: 609.75 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  420.167 μs (220 allocations: 1.86 MiB)\n"
     ]
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "f = (W, _) -> sum(exp(W))\n",
    "\n",
    "@btime tangent_approx(f, $W_bench, $edges_bench, 0.01) \n",
    "@btime finite_diff(f, $W_bench, $edges_bench, 0.1) \n",
    "@btime forward_diff_j($W_bench, $edges_bench) \n",
    "@btime forward_diff_jvp($W_bench, $edges_bench) \n",
    "@btime frechet_block_enlarge($W_bench, $edges_bench) \n",
    "@btime frechet_algo($W_bench, $edges_bench);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension to full objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "W_Y, D, A_init = load_weight_test_data()\n",
    "A_Y = Float64.(W_Y .> 0);\n",
    "α = 0.01\n",
    "ω = 0.9\n",
    "m_seed = Int(sum(A_init))\n",
    "resolution = 0.01\n",
    "steps = 5\n",
    "\n",
    "zero_indices = findall(==(0), triu(A_init, 1))\n",
    "edges_to_add = sample(zero_indices, m_max-m_seed; replace = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_tangent(m_max)\n",
    "    A_current = copy(A_init)\n",
    "    W_current = copy(A_init)\n",
    "    edge_indices = Vector{CartesianIndex{2}}()\n",
    "    f = (W, edge_idx) -> (sum(exp(W))  * D[edge_idx])^ω\n",
    "\n",
    "    for m in 1:m_max\n",
    "        # Get the edge\n",
    "        edge_idx = edges_to_add[m-m_seed]\n",
    "        rev_idx = CartesianIndex(edge_idx[2], edge_idx[1])\n",
    "        A_current[edge_idx] = W_current[edge_idx] =  1\n",
    "        A_current[rev_idx] = W_current[rev_idx] =  1    \n",
    "        push!(edge_indices, edge_idx)\n",
    "        \n",
    "        tangent_d = tangent_approx(f, W_current, edge_indices, resolution, steps)\n",
    "        # println(round.(tangent_d, digits=2))\n",
    "\n",
    "        for (i_edge, edge) in enumerate(edge_indices)\n",
    "            update = max(0, (α * tangent_d[i_edge]))\n",
    "            W_current[edge] -= update\n",
    "            W_current[CartesianIndex(edge[2], edge[1])] -= update\n",
    "        end\n",
    "    end\n",
    "    return W_current\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_frechet_algo(m_max)\n",
    "    A_current = copy(A_init)\n",
    "    W_current = copy(A_init)\n",
    "    edge_indices = Vector{CartesianIndex{2}}()\n",
    "\n",
    "    for m in 1:m_max\n",
    "        # Get the edge\n",
    "        edge_idx = edges_to_add[m-m_seed]\n",
    "        rev_idx = CartesianIndex(edge_idx[2], edge_idx[1])\n",
    "        A_current[edge_idx] = W_current[edge_idx] =  1\n",
    "        A_current[rev_idx] = W_current[rev_idx] =  1    \n",
    "        push!(edge_indices, edge_idx)\n",
    "        \n",
    "        current = sum(exp(W_current))\n",
    "        frechet_d = frechet_algo(W_current, edge_indices)\n",
    "        frechet_d = [ω * D[edge]^ω  * frechet_d[i_edge] * current^(ω-1) for (i_edge, edge) in enumerate(edge_indices)]\n",
    "        # println(round.(frechet_d, digits=2))\n",
    "\n",
    "        for (i_edge, edge) in enumerate(edge_indices)\n",
    "            update = max(0, (α * frechet_d[i_edge]))\n",
    "            W_current[edge] -= update\n",
    "            W_current[CartesianIndex(edge[2], edge[1])] -= update\n",
    "        end\n",
    "    end\n",
    "    return W_current\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12.316652 seconds (66.69 k allocations: 677.437 MiB, 0.36% gc time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.458034 seconds (8.41 k allocations: 62.937 MiB, 1.11% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004211385608813206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_max = 20\n",
    "@time W_res_tangent = test_tangent(m_max);\n",
    "@time W_res_frechet = test_frechet_algo(m_max);\n",
    "sum(abs.(W_res_tangent .- W_res_frechet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
