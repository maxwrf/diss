{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Differentiation of matrix exp for weighted GNMs <a name=\"intro\"></a>\n",
    "From: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "This is mostly concerned with finding the fastest way to compute the derivative of the matrix exponential.\n",
    "$$\n",
    "f(w_{i,j}) = (c_{i,k} * d_{i,j})^{\\omega}\n",
    "$$\n",
    "Where, the communicability matrix is defined as follows. This captures the proportion of signals, that propagate randomly from node i would reach node j over an infinite time horizon.\n",
    "$$\n",
    "c_{i,j} = e^{s^{-1/2}w_{i,j}s^{-1/2}}\n",
    "$$\n",
    "\n",
    "Finally, the update rule is given as:\n",
    "$$\n",
    "w_{i,j} = w_{i,j} - \\alpha f'(w_{i,j})\n",
    "$$\n",
    "\n",
    "**Question**<br>\n",
    "In the code implementation this seems to be bound to zero in Line 110, but there is no upper bound, should it be normalized to range [0,1]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_weight_test_data (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "using StatsBase: sample\n",
    "include(\"test_data.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3Ã—3 Matrix{Float64}:\n",
       " 0.0  0.8  0.0\n",
       " 0.8  0.0  0.2\n",
       " 0.0  0.2  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "demo_edge = [CartesianIndex(1, 2)]\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods <a name=\"methods\"></a>\n",
    "Here we will work on different approaches as to compute the derivative.\n",
    "In this demonstration, we will simply work on the matrix exponential differentiation, as the rest of the operation is trivial and computationally cheap.\n",
    "\n",
    "For:\n",
    "$$\n",
    "f(W)=e^{W}\n",
    "$$\n",
    "Find\n",
    "$$\n",
    "\\frac{\\delta f}{\\delta W_{i,j}}\n",
    "$$\n",
    "Or more specifically \n",
    "$$\n",
    "f(W)=\\sum_{i,j} e^{W}\n",
    "$$\n",
    "\n",
    "But this also simply boils down to the evaluation of the derivative at one (a matrix of ones at the same size as W for the jacobian vector product)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original tangent approximation <a name=\"org-tangent\"></a>\n",
    "\n",
    "Here we are computing the derivative for a single edge. We will use the tangent approximation to compute the derivative. \n",
    "This is the approach used in the Akarca paper and available in the repo, as can be seen below this will not produce the correct result.\n",
    "\n",
    "**Problem 1**<br>\n",
    "As far as I understand, there is a small but significant error in the implementation provided at https://github.com/DanAkarca/weighted_generative_models/blob/main/weighted_generative_model.m in line 197.\n",
    "When fitting the first-order polynomial, the independent variable should not be the range of the differences to the edge, which have been used, but should be the actual values, the edge is taking on.\n",
    "\n",
    "**Problem 2**<br>\n",
    "The second problem is that we will often need to compute the derivative with respect to a value in the matrix, which is currently zero. (This happens easily as the value for any weight has a lower bound of zero which is often hit).\n",
    "Now the approach implemented in the code builds a range of x value by using a value for the number of repititions and resolutions for the tangent approximations, but it multiples the resolution with the current x-value, which will produce a vector of zeros whenever the current value is zero, which will always produce a zero slope and zero derivative approximations - which is wrong.\n",
    "\n",
    "**Problem 2**\n",
    "In the same implementation the derivative is approximated from the tangent approximation after nudging the edge (i,j) as well as (j,i).\n",
    "In a sense we are computing the derivative with respect to both (i,j) and (j,i) at once.\n",
    "First of all, this will create computational overhead, as the derivatives (i.e., the jacobian) will also always be symmetric.\n",
    "Secondly, this will create a problem when we are trying to compute the gradient, which we currently simply take as the the derivative.\n",
    "But then we update both elements in the W matrix with the same gradient which is the derivative with respect to nudging both (i,j) and (j,i) - thus our update to the objective function is overshooting.\n",
    "\n",
    "See line 208 and 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03861909629029788], [0.830380354061412])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted tangent approximation <a name=\"adapted-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.4132596676512077], [2.4141043556312676])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tangent_approx(f::Function, W::Matrix{Float64}, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        sign_edge = sign(edge_val) == 0 ? 1 : sign(edge_val)\n",
    "        reps = [edge_val + sign_edge * (max(abs(edge_val), 1e-3) * i) for i in rep_vec]\n",
    "        \n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = rep \n",
    "            sum_comm[i_rep] = f(W_copy, edge_idx)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(reps, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = (W, _) -> sum(exp(W))\n",
    "tangent_approx(f, W, demo_edge, 0.01), tangent_approx(f, W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference method <a name=\"finite-diff\"></a>\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "At the moment, this is a significantly faster but less accurate method than the tangent approximation.\n",
    "Only implemented to be potentially extended to complex step approximation.\n",
    "\n",
    "References:<br>\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.413251884607348], [2.4135507160903513])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function finite_diff(f, W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx] = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = W -> sum(exp(W))\n",
    "finite_diff(f, W, demo_edge, 0.01), finite_diff(f, W, demo_edge,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.4132511356618336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jâ‚“ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = sum(Jâ‚“)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661834"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    tangent = ones(size(W))\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges]\n",
    "end\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrÃ©chet: Block enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_block_enlarge(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(W))\n",
    "    n = size(W, 1)\n",
    "    M = [W E; zeros(size(W)) W]\n",
    "    expm_M = exp(M) \n",
    "    frechet_AE = expm_M[1:n, n+1:end]\n",
    "    return frechet_AE[edges]\n",
    "end\n",
    "\n",
    "frechet_block_enlarge(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrÃ©chet: Scaling-Pade-Squaring\n",
    "\n",
    "The algorithm:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{for } m = [3, 5, 7, 9] \\\\\n",
    "\\quad \\quad \\text{if } ||A||_{1} \\leq \\theta_{m} \\\\\n",
    "\\quad \\quad \\quad \\quad r_{m}(A) \\quad \\text{ \\textit{form Pade approximant to A}} \\\\\n",
    "\\quad \\quad \\quad \\quad U, V     \\quad \\text{ \\textit{Evaluate U and V, see below for }} r_{13} \\\\\n",
    "\\quad \\quad \\quad \\quad s=0 \\\\\n",
    "\\quad \\quad \\quad \\quad   \\text{break} \\\\\n",
    "\\quad \\quad \\text{end} \\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{if } ||A||_{1} \\geq  \\theta_{m[-1]} \\\\\n",
    "\\quad \\quad s = \\text{ceil}(\\log_{2}(||A||_{1}/ \\theta_{13})) \\\\\n",
    "\\quad \\quad A = A/2^{s} \\\\\n",
    "\\quad \\quad A_{2} = A^{2}, A_{4} = A_{2}^{2}, A_{6} = A_{2}A_{4}\\\\ \n",
    "\\quad \\quad U = A[A_{6}(b_{13} A_{6} + b_{11} A_{4} + b_{9} A_{2}) + b_{7} A_{6} + b_{5} A_{4} + b_{3} A_{2} + b_{1} I]\\\\\n",
    "\\quad \\quad V = A_{6}(b_{12}A_{6} + b_{10}A_{4} + b_{8}A_{2}) + b_{6}A_{6} + b_{4}A_{4} + b_{2}A_{2} + b_{0}I\\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{Solve } (-U+V)r_{m}(A) = U + V \\text{ for } r_{m} \\\\\n",
    "\\text{for } k = 1:s \\\\\n",
    "\\quad \\quad r_{m} = r_{m}*r_{m} \\\\\n",
    "\\text{end} \\\\\n",
    "\\text{return } r_{m} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "References:<br>\n",
    "Higham (2008), Functions of Matrices - Theory and Computation\", Chapter 10, Algorithm 10.20 <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet <br>\n",
    "https://rdrr.io/cran/expm/man/expmFrechet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function _diff_pade3(A, E, ident)\n",
    "    b = (120.0, 60.0, 12.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    U = A * (b[4] * A2 + b[2] * ident)\n",
    "    V = b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[4] * M2) + E * (b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "    \n",
    "function _diff_pade5(A, E, ident)\n",
    "    b = (30240.0, 15120.0, 3360.0, 420.0, 30.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    U = A * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[6] * M4 + b[4] * M2) + E * (b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade7(A, E, ident)\n",
    "    b = (17297280.0, 8648640.0, 1995840.0, 277200.0, 25200.0, 1512.0, 56.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    U = A * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade9(A, E, ident)\n",
    "    b = (17643225600.0, 8821612800.0, 2075673600.0, 302702400.0, 30270240.0, \n",
    "         2162160.0, 110880.0, 3960.0, 90.0, 1.0)\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    A8 = A4 * A4\n",
    "    M8 = A4 * M4 + M4 * A4\n",
    "    U = A * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    V = b[9] * A8 + b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    Lu = A * (b[10] * M8 + b[8] * M6 + b[6] * M4 + b[4] * M2) + E * (b[10] * A8 + b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident)\n",
    "    Lv = b[9] * M8 + b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "function _diff_pade13(A, E, ident)\n",
    "    # pade order 13\n",
    "    A2 = A * A\n",
    "    M2 = A * E + E * A\n",
    "    A4 = A2 * A2\n",
    "    M4 = A2 * M2 + M2 * A2\n",
    "    A6 = A2 * A4\n",
    "    M6 = A4 * M2 + M4 * A2\n",
    "    b = (64764752532480000., 32382376266240000., 7771770303897600.,\n",
    "            1187353796428800., 129060195264000., 10559470521600.,\n",
    "            670442572800., 33522128640., 1323241920., 40840800., 960960.,\n",
    "            16380., 182., 1.)\n",
    "    W1 = b[14] * A6 + b[12] * A4 + b[10] * A2\n",
    "    W2 = b[8] * A6 + b[6] * A4 + b[4] * A2 + b[2] * ident\n",
    "    Z1 = b[13] * A6 + b[11] * A4 + b[9] * A2\n",
    "    Z2 = b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident\n",
    "    W = A6 * W1 + W2\n",
    "    U = A * W\n",
    "    V = A6 * Z1 + Z2\n",
    "    Lw1 = b[14] * M6 + b[12] * M4 + b[10] * M2\n",
    "    Lw2 = b[8] * M6 + b[6] * M4 + b[4] * M2\n",
    "    Lz1 = b[13] * M6 + b[11] * M4 + b[9] * M2\n",
    "    Lz2 = b[7] * M6 + b[5] * M4 + b[3] * M2\n",
    "    Lw = A6 * Lw1 + M6 * W1 + Lw2\n",
    "    Lu = A * Lw + E * W\n",
    "    Lv = A6 * Lz1 + M6 * Z1 + Lz2\n",
    "    return U, V, Lu, Lv\n",
    "end\n",
    "\n",
    "ell_table_61 = (nothing, 2.11e-8, 3.56e-4, 1.08e-2, 6.49e-2, 2.00e-1, 4.37e-1,\n",
    "        7.83e-1, 1.23e0, 1.78e0, 2.42e0, 3.13e0, 3.90e0, 4.74e0, 5.63e0,\n",
    "        6.56e0, 7.52e0, 8.53e0, 9.56e0, 1.06e1, 1.17e1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_algo(A::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(A))\n",
    "    n = size(A, 1)\n",
    "    s = nothing\n",
    "    ident = Matrix{Float64}(I, n, n)\n",
    "    A_norm_1 = norm(A, 1)\n",
    "    m_pade_pairs = [(3, _diff_pade3),(5, _diff_pade5),(7, _diff_pade7),(9, _diff_pade9)]\n",
    "    for (m, pade) in m_pade_pairs\n",
    "        if A_norm_1 <= ell_table_61[m]\n",
    "            U, V, Lu, Lv = pade(A, E, ident)\n",
    "            s = 0\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if s == nothing\n",
    "        # scaling\n",
    "        s = max(0, ceil(Int, log2(A_norm_1 / ell_table_61[13])))\n",
    "        A *= 2.0^-s\n",
    "        E *= 2.0^-s\n",
    "        U, V, Lu, Lv = _diff_pade13(A, E, ident)\n",
    "    end\n",
    "    \n",
    "    # factor once and solve twice\n",
    "    lu_piv = lu(-U + V)\n",
    "    R = lu_piv \\ (U + V)\n",
    "    L = lu_piv \\ (Lu + Lv + ((Lu - Lv) * R))\n",
    "    \n",
    "    # repeated squaring\n",
    "    for k in 1:s\n",
    "        L = R * L + L * R\n",
    "        R= R * R\n",
    "    end\n",
    "    return L[edges]\n",
    "end\n",
    "\n",
    "frechet_algo(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking <a name=\"benchmarking\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  490.936 ms (64371 allocations: 362.92 MiB)\n",
      "  85.713 ms (7423 allocations: 58.01 MiB)\n",
      "  1.721 s (5884 allocations: 311.96 MiB)\n",
      "  1.503 ms (20 allocations: 315.22 KiB)\n",
      "  2.591 ms (24 allocations: 609.55 KiB)\n",
      "  385.708 Î¼s (211 allocations: 1.78 MiB)\n"
     ]
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "f = (W, _) -> sum(exp(W))\n",
    "\n",
    "@btime tangent_approx(f, $W_bench, $edges_bench, 0.01) \n",
    "@btime finite_diff(f, $W_bench, $edges_bench, 0.1) \n",
    "@btime forward_diff_j($W_bench, $edges_bench) \n",
    "@btime forward_diff_jvp($W_bench, $edges_bench) \n",
    "@btime frechet_block_enlarge($W_bench, $edges_bench) \n",
    "@btime frechet_algo($W_bench, $edges_bench);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension to full objective function\n",
    "\n",
    "Currently I can successfully compute the objective function derivative without normalization.\n",
    "This brings a tremendous speedup, demonstrated below on the dataset provided by Akarca in the demo repo.\n",
    "Taking the derivative of the normalized version turns out to be more tricky than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "W_Y, D, A_init = load_weight_test_data()\n",
    "A_Y = Float64.(W_Y .> 0);\n",
    "Î± = 0.01\n",
    "Ï‰ = 0.9\n",
    "Ïµ = 1e-5\n",
    "m_seed = Int(sum(A_init))\n",
    "m_all = Int(sum(A_Y))\n",
    "resolution = 0.01\n",
    "steps = 5\n",
    "\n",
    "zero_indices = (findall(==(1), triu(abs.(A_init .- 1), 1)))\n",
    "edges_to_add = sample(zero_indices, m_all-m_seed; replace = false);\n",
    "\n",
    "function obj_func_auto(W::Matrix{Float64})\n",
    "    return sum(exponential!(copyto!(similar(W), W), ExpMethodGeneric()))\n",
    "end;\n",
    "\n",
    "function norm_obj_func_auto(W::Matrix{Float64})\n",
    "    node_strengths = dropdims(sum(W, dims=2), dims=2)\n",
    "    node_strengths[node_strengths.==0] .= 1e-5\n",
    "    S = sqrt(inv(Diagonal(node_strengths)))\n",
    "    return sum(exponential!(copyto!(similar(W), S * W * S), ExpMethodGeneric()))\n",
    "end;\n",
    "\n",
    "function obj_func_tangent(W::Matrix{Float64}, edge_idx)\n",
    "    return (sum(exp(W))  * D[edge_idx])^Ï‰\n",
    "end;\n",
    "\n",
    "function norm_obj_func_tangent(W::Matrix{Float64}, edge_idx)\n",
    "    node_strengths = dropdims(sum(W, dims=2), dims=2)\n",
    "    node_strengths[node_strengths.==0] .= 1e-5\n",
    "    S = sqrt(inv(Diagonal(node_strengths)))\n",
    "    return (sum(exp(S * W * S)) * D[edge_idx])^Ï‰\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_model(\n",
    "    model::String,\n",
    "    obj_func::Union{Function, Nothing},\n",
    "    A_init::Matrix{Float64}, \n",
    "    m_max::Int, \n",
    "    verbose::Bool, \n",
    "    normalize::Bool)\n",
    "    A_current = copy(A_init)\n",
    "    W_current = copy(A_init)\n",
    "\n",
    "    for m in 1:m_max\n",
    "        # Get the edge, order of added edges is fixed\n",
    "        edge_idx = edges_to_add[m-m_seed]\n",
    "        rev_idx = CartesianIndex(edge_idx[2], edge_idx[1])\n",
    "        A_current[edge_idx] = W_current[edge_idx] =  1\n",
    "        A_current[rev_idx] = W_current[rev_idx] =  1    \n",
    "        edge_indices = findall(!=(0), triu(A_current, 1))\n",
    "        \n",
    "        # Compute the derivative\n",
    "        if model == \"tangent\"\n",
    "            derivative = tangent_approx(obj_func, W_current, edge_indices, resolution, steps)\n",
    "        elseif model == \"auto-diff\"\n",
    "            println(sum(W_current))\n",
    "            println(typeof(W_current))\n",
    "            auto_d = ForwardDiff.gradient(obj_func, W_current)\n",
    "            prinln(size(auto_d))\n",
    "            println(round.(auto_d, digits=6))\n",
    "            derivative = [Ï‰ * D[edge]^Ï‰  * auto_d[i_edge] * current_Y^(Ï‰-1) for (i_edge, edge) in enumerate(edge_indices)]\n",
    "            throw(ArgumentError(\"Model not supported\"))\n",
    "        elseif model == \"frechet-algo\"\n",
    "            current_Y = sum(exp(W_current))\n",
    "            frechet_d = frechet_algo(W_current, edge_indices)\n",
    "            derivative = [Ï‰ * D[edge]^Ï‰  * frechet_d[i_edge] * current_Y^(Ï‰-1) for (i_edge, edge) in enumerate(edge_indices)]\n",
    "        else\n",
    "            throw(ArgumentError(\"Model not supported\"))\n",
    "        end\n",
    "\n",
    "        # Print the derivative for the at the current iteration, all edges added so far\n",
    "        if verbose\n",
    "            println(round.(derivative, digits=6))\n",
    "        end\n",
    "\n",
    "        # Update W matrix\n",
    "        for (i_edge, edge) in enumerate(edge_indices)\n",
    "            W_current[edge] -= (Î± * derivative[i_edge])\n",
    "            W_current[edge] = max(0, W_current[edge])\n",
    "            W_current[CartesianIndex(edge[2], edge[1])] = W_current[edge]\n",
    "        end\n",
    "    end\n",
    "    return W_current\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "Matrix{Float64}\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching (::var\"#20#21\")(::Matrix{ForwardDiff.Dual{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12}})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  (::var\"#20#21\")(::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[36mMain\u001b[39m \u001b[90m\u001b[4mIn[11]:14\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::var\"#20#21\")(::Matrix{ForwardDiff.Dual{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12}})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  (::var\"#20#21\")(::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[36mMain\u001b[39m \u001b[90m\u001b[4mIn[11]:14\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      " [1] chunk_mode_gradient(f::var\"#20#21\", x::Matrix{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12}}})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/vXysl/src/gradient.jl:123",
      " [2] gradient(f::Function, x::Matrix{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12}}}, ::Val{true})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/vXysl/src/gradient.jl:0",
      " [3] gradient(f::Function, x::Matrix{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{var\"#20#21\", Float64}, Float64, 12}}})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/vXysl/src/gradient.jl:17",
      " [4] gradient(f::Function, x::Matrix{Float64})",
      "   @ ForwardDiff ~/.julia/packages/ForwardDiff/vXysl/src/gradient.jl:17",
      " [5] test_model(model::String, obj_func::typeof(norm_obj_func_auto), A_init::Matrix{Float64}, m_max::Int64, verbose::Bool, normalize::Bool)",
      "   @ Main ./In[22]:25",
      " [6] top-level scope",
      "   @ ./timing.jl:273 [inlined]",
      " [7] top-level scope",
      "   @ ./In[25]:0"
     ]
    }
   ],
   "source": [
    "@time W_res_auto = test_model(\"auto-diff\", obj_fu, A_init, 10, true, false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101.454088]\n",
      "[117.231767, 37.322597]\n",
      "[43.126838, 64.13074, 71.883459]\n",
      "[43.087831, 43.058883, 34.998296, 133.995615]\n",
      "[43.126838, 37.322597, 26.444251, 66.617135, 49.338492]\n",
      "[43.046904, 37.253421, 26.395238, 56.717627, 49.247045, 38.198427]\n",
      "[43.024912, 37.234389, 26.381753, 33.833792, 49.221885, 129.767734, 22.844578]\n",
      "[43.031256, 37.239879, 26.385643, 32.027406, 49.229144, 47.745567, 48.704336, 30.02135]\n",
      "[43.032361, 37.240835, 26.38632, 25.792979, 49.230407, 89.030037, 47.746792, 26.629555, 17.492573]\n",
      "[43.079351, 37.281501, 26.415133, 24.479908, 84.684085, 80.841006, 36.589357, 47.79893, 19.655044, 13.982765]\n",
      "  3.177863 seconds (303.10 k allocations: 213.664 MiB, 0.74% gc time, 5.16% compilation time)\n",
      "[101.453338]\n",
      "[117.230899, 37.322597]\n",
      "[43.126838, 64.13074, 71.882927]\n",
      "[43.08783, 43.059002, 34.998479, 133.99462]\n",
      "[43.126838, 37.322597, 26.444251, 66.616642, 49.338492]\n",
      "[43.046903, 37.25342, 26.395237, 56.717872, 49.247044, 38.198245]\n",
      "[43.024911, 37.234389, 26.381752, 33.833825, 49.221885, 129.766771, 22.844588]\n",
      "[43.031256, 37.239879, 26.385643, 32.027443, 49.229143, 47.745567, 48.704045, 30.021391]\n",
      "[43.03236, 37.240834, 26.386319, 25.793, 49.230406, 89.029375, 47.746791, 26.629629, 17.492621]\n",
      "[43.079349, 37.2815, 26.415132, 24.479908, 84.684083, 80.840405, 36.589598, 47.798929, 19.655085, 13.982781]\n",
      "  0.300334 seconds (44.38 k allocations: 31.488 MiB, 1.29% gc time, 6.69% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5537775412743926e-5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time W_res_tangent = test_model(\"tangent\", obj_func_tangent, A_init, 10, true, false);\n",
    "@time W_res_frechet = test_model(\"frechet-algo\", nothing, A_init, 10, true, false);\n",
    "sum(abs.(W_res_tangent .- W_res_frechet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4834167279944077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(W_res_frechet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
