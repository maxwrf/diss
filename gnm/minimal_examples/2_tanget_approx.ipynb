{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference in tangent approximation results\n",
    "Ref: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "As this is a minimal example, we will simply concern ourselves with taking the derivative of\n",
    "$$\n",
    "f(W)=\\sum\\sum e^{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "\n",
    "include(\"example_imports.jl\");\n",
    "\n",
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "     \n",
    "demo_edge = [CartesianIndex(1, 2)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangent approximation (as I think is implemented)\n",
    "Results will change for example with resolution, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03861909629029788], [0.3932448105709426])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted tangent approximation <a name=\"adapted-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.4132596676512077], [2.4141043556312676])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tangent_approx(\n",
    "    f::Function, \n",
    "    W::Matrix{Float64},\n",
    "    edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    \"\"\"\n",
    "    Compute the derivative of any function f with respec to the vector of edges provided\n",
    "    Will use n = steps number of nudges of current w_{edge idx} by (1+resolution)\n",
    "    \"\"\"\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        sign_edge = sign(edge_val) == 0 ? 1 : sign(edge_val)\n",
    "        x = [edge_val + sign_edge * (max(abs(edge_val), 1e-3) * i) for i in rep_vec]\n",
    "        \n",
    "        # For each nudge save difference in communicability \n",
    "        y = zeros(length(x))\n",
    "        for (i_rep, rep) in enumerate(x)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = rep \n",
    "            y[i_rep] = f(W_copy, edge_idx)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(x, y, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end;\n",
    "\n",
    "f = (W, _) -> sum(exp(W))\n",
    "tangent_approx(f, W, demo_edge, 0.01), tangent_approx(f, W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to compute the derivative fast (not directly relevant to our discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite difference method <a name=\"finite-diff\"></a>\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "Only implemented to be potentially extended to complex step approximation.\n",
    "\n",
    "References:<br>\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.413251884607348], [2.4133260303517945])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function finite_diff(f, W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx] = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = W -> sum(exp(W))\n",
    "finite_diff(f, W, demo_edge, 0.01), finite_diff(f, W, demo_edge,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.4132511356618336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jₓ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = sum(Jₓ)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661834"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # note, that this is only possible because we have symmetry\n",
    "    tangent = ones(size(W))\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges]\n",
    "end\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fréchet: Block enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_block_enlarge(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(W))\n",
    "    n = size(W, 1)\n",
    "    M = [W E; zeros(size(W)) W]\n",
    "    expm_M = exp(M) \n",
    "    frechet_AE = expm_M[1:n, n+1:end]\n",
    "    return frechet_AE[edges]\n",
    "end\n",
    "\n",
    "frechet_block_enlarge(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fréchet: Scaling-Pade-Squaring\n",
    "\n",
    "The algorithm:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{for } m = [3, 5, 7, 9] \\\\\n",
    "\\quad \\quad \\text{if } ||A||_{1} \\leq \\theta_{m} \\\\\n",
    "\\quad \\quad \\quad \\quad r_{m}(A) \\quad \\text{ \\textit{form Pade approximant to A}} \\\\\n",
    "\\quad \\quad \\quad \\quad U, V     \\quad \\text{ \\textit{Evaluate U and V, see below for }} r_{13} \\\\\n",
    "\\quad \\quad \\quad \\quad s=0 \\\\\n",
    "\\quad \\quad \\quad \\quad   \\text{break} \\\\\n",
    "\\quad \\quad \\text{end} \\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{if } ||A||_{1} \\geq  \\theta_{m[-1]} \\\\\n",
    "\\quad \\quad s = \\text{ceil}(\\log_{2}(||A||_{1}/ \\theta_{13})) \\\\\n",
    "\\quad \\quad A = A/2^{s} \\\\\n",
    "\\quad \\quad A_{2} = A^{2}, A_{4} = A_{2}^{2}, A_{6} = A_{2}A_{4}\\\\ \n",
    "\\quad \\quad U = A[A_{6}(b_{13} A_{6} + b_{11} A_{4} + b_{9} A_{2}) + b_{7} A_{6} + b_{5} A_{4} + b_{3} A_{2} + b_{1} I]\\\\\n",
    "\\quad \\quad V = A_{6}(b_{12}A_{6} + b_{10}A_{4} + b_{8}A_{2}) + b_{6}A_{6} + b_{4}A_{4} + b_{2}A_{2} + b_{0}I\\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{Solve } (-U+V)r_{m}(A) = U + V \\text{ for } r_{m} \\\\\n",
    "\\text{for } k = 1:s \\\\\n",
    "\\quad \\quad r_{m} = r_{m}*r_{m} \\\\\n",
    "\\text{end} \\\\\n",
    "\\text{return } r_{m} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "References:<br>\n",
    "Higham (2008), Functions of Matrices - Theory and Computation\", Chapter 10, Algorithm 10.20 <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet <br>\n",
    "https://rdrr.io/cran/expm/man/expmFrechet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_algo(A::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(A))\n",
    "    n = size(A, 1)\n",
    "    s = nothing\n",
    "    ident = Matrix{Float64}(I, n, n)\n",
    "    A_norm_1 = norm(A, 1)\n",
    "    m_pade_pairs = [(3, _diff_pade3),(5, _diff_pade5),(7, _diff_pade7),(9, _diff_pade9)]\n",
    "    for (m, pade) in m_pade_pairs\n",
    "        if A_norm_1 <= ell_table_61[m]\n",
    "            U, V, Lu, Lv = pade(A, E, ident)\n",
    "            s = 0\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if s == nothing\n",
    "        # scaling\n",
    "        s = max(0, ceil(Int, log2(A_norm_1 / ell_table_61[13])))\n",
    "        A *= 2.0^-s\n",
    "        E *= 2.0^-s\n",
    "        U, V, Lu, Lv = _diff_pade13(A, E, ident)\n",
    "    end\n",
    "    \n",
    "    # factor once and solve twice\n",
    "    lu_piv = lu(-U + V)\n",
    "    R = lu_piv \\ (U + V)\n",
    "    L = lu_piv \\ (Lu + Lv + ((Lu - Lv) * R))\n",
    "    \n",
    "    # repeated squaring\n",
    "    for k in 1:s\n",
    "        L = R * L + L * R\n",
    "        R= R * R\n",
    "    end\n",
    "    return L[edges]\n",
    "end\n",
    "\n",
    "frechet_algo(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking <a name=\"benchmarking\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  466.782 ms (63587 allocations: 358.38 MiB)\n",
      "  80.744 ms (7337 allocations: 57.29 MiB)\n",
      "  1.723 s (5884 allocations: 311.91 MiB)\n",
      "  1.503 ms (20 allocations: 315.22 KiB)\n",
      "  1.969 ms (24 allocations: 609.55 KiB)\n",
      "  396.000 μs (211 allocations: 1.78 MiB)\n"
     ]
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "f = (W, _) -> sum(exp(W))\n",
    "\n",
    "@btime tangent_approx(f, $W_bench, $edges_bench, 0.01) \n",
    "@btime finite_diff(f, $W_bench, $edges_bench, 0.1) \n",
    "@btime forward_diff_j($W_bench, $edges_bench) \n",
    "@btime forward_diff_jvp($W_bench, $edges_bench) \n",
    "@btime frechet_block_enlarge($W_bench, $edges_bench) \n",
    "@btime frechet_algo($W_bench, $edges_bench);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
