{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation of matrix exp for weighted GNMs <a name=\"intro\"></a>\n",
    "From: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "This is mostly concerned with finding the fastest way to compute the derivative of the matrix exponential.\n",
    "$$\n",
    "f(w_{i,j}) = \\sum \\sum (c_{i,k} * d_{i,j})^{\\omega}\n",
    "$$\n",
    "Where, the communicability matrix is defined as follows. This captures the proportion of signals, that propagate randomly from node i would reach node j over an infinite time horizon.\n",
    "$$\n",
    "c_{i,j} = e^{s^{-1/2}w_{i,j}s^{-1/2}}\n",
    "$$\n",
    "\n",
    "Finally, the update rule is given as:\n",
    "$$\n",
    "w_{i,j} = w_{i,j} - \\alpha f'(w_{i,j})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "\n",
    "include(\"example_imports.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.0  0.8  0.0\n",
       " 0.8  0.0  0.2\n",
       " 0.0  0.2  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "demo_edge = [CartesianIndex(1, 2)]\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods <a name=\"methods\"></a>\n",
    "Here we will work on different approaches as to compute the derivative.\n",
    "In this demonstration, we will simply work on the matrix exponential differentiation, as the rest of the operation is trivial and computationally cheap.\n",
    "\n",
    "For:\n",
    "$$\n",
    "f(W)=e^{W}\n",
    "$$\n",
    "Find\n",
    "$$\n",
    "\\frac{\\delta f}{\\delta W_{i,j}}\n",
    "$$\n",
    "Or more specifically \n",
    "$$\n",
    "f(W)=\\sum_{i,j} e^{W}\n",
    "$$\n",
    "\n",
    "But this also simply boils down to the evaluation of the derivative at one (a matrix of ones at the same size as W for the jacobian vector product)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original tangent approximation <a name=\"org-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03861909629029788], [0.3932448105709426])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted tangent approximation <a name=\"adapted-tangent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.4132596676512077], [2.4141043556312676])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tangent_approx(f::Function, W::Matrix{Float64}, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, steps = 5)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(range(-steps*resolution, steps*resolution, step=resolution))\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        sign_edge = sign(edge_val) == 0 ? 1 : sign(edge_val)\n",
    "        reps = [edge_val + sign_edge * (max(abs(edge_val), 1e-3) * i) for i in rep_vec]\n",
    "        \n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = rep \n",
    "            sum_comm[i_rep] = f(W_copy, edge_idx)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(reps, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = (W, _) -> sum(exp(W))\n",
    "tangent_approx(f, W, demo_edge, 0.01), tangent_approx(f, W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference method <a name=\"finite-diff\"></a>\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "Only implemented to be potentially extended to complex step approximation.\n",
    "\n",
    "References:<br>\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.413251884607348], [2.4133260303517945])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function finite_diff(f, W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx] = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "f = W -> sum(exp(W))\n",
    "finite_diff(f, W, demo_edge, 0.01), finite_diff(f, W, demo_edge,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.4132511356618336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jₓ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = sum(Jₓ)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661834"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # note, that this is only possible because we have symmetry\n",
    "    tangent = ones(size(W))\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges]\n",
    "end\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Block enlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_block_enlarge(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(W))\n",
    "    n = size(W, 1)\n",
    "    M = [W E; zeros(size(W)) W]\n",
    "    expm_M = exp(M) \n",
    "    frechet_AE = expm_M[1:n, n+1:end]\n",
    "    return frechet_AE[edges]\n",
    "end\n",
    "\n",
    "frechet_block_enlarge(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet: Scaling-Pade-Squaring\n",
    "\n",
    "The algorithm:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text{for } m = [3, 5, 7, 9] \\\\\n",
    "\\quad \\quad \\text{if } ||A||_{1} \\leq \\theta_{m} \\\\\n",
    "\\quad \\quad \\quad \\quad r_{m}(A) \\quad \\text{ \\textit{form Pade approximant to A}} \\\\\n",
    "\\quad \\quad \\quad \\quad U, V     \\quad \\text{ \\textit{Evaluate U and V, see below for }} r_{13} \\\\\n",
    "\\quad \\quad \\quad \\quad s=0 \\\\\n",
    "\\quad \\quad \\quad \\quad   \\text{break} \\\\\n",
    "\\quad \\quad \\text{end} \\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{if } ||A||_{1} \\geq  \\theta_{m[-1]} \\\\\n",
    "\\quad \\quad s = \\text{ceil}(\\log_{2}(||A||_{1}/ \\theta_{13})) \\\\\n",
    "\\quad \\quad A = A/2^{s} \\\\\n",
    "\\quad \\quad A_{2} = A^{2}, A_{4} = A_{2}^{2}, A_{6} = A_{2}A_{4}\\\\ \n",
    "\\quad \\quad U = A[A_{6}(b_{13} A_{6} + b_{11} A_{4} + b_{9} A_{2}) + b_{7} A_{6} + b_{5} A_{4} + b_{3} A_{2} + b_{1} I]\\\\\n",
    "\\quad \\quad V = A_{6}(b_{12}A_{6} + b_{10}A_{4} + b_{8}A_{2}) + b_{6}A_{6} + b_{4}A_{4} + b_{2}A_{2} + b_{0}I\\\\\n",
    "\\text{end} \\\\\n",
    "\n",
    "\\text{Solve } (-U+V)r_{m}(A) = U + V \\text{ for } r_{m} \\\\\n",
    "\\text{for } k = 1:s \\\\\n",
    "\\quad \\quad r_{m} = r_{m}*r_{m} \\\\\n",
    "\\text{end} \\\\\n",
    "\\text{return } r_{m} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "References:<br>\n",
    "Higham (2008), Functions of Matrices - Theory and Computation\", Chapter 10, Algorithm 10.20 <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html#scipy.linalg.expm_frechet <br>\n",
    "https://rdrr.io/cran/expm/man/expmFrechet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.413251135661833"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function frechet_algo(A::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    E = ones(size(A))\n",
    "    n = size(A, 1)\n",
    "    s = nothing\n",
    "    ident = Matrix{Float64}(I, n, n)\n",
    "    A_norm_1 = norm(A, 1)\n",
    "    m_pade_pairs = [(3, _diff_pade3),(5, _diff_pade5),(7, _diff_pade7),(9, _diff_pade9)]\n",
    "    for (m, pade) in m_pade_pairs\n",
    "        if A_norm_1 <= ell_table_61[m]\n",
    "            U, V, Lu, Lv = pade(A, E, ident)\n",
    "            s = 0\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if s == nothing\n",
    "        # scaling\n",
    "        s = max(0, ceil(Int, log2(A_norm_1 / ell_table_61[13])))\n",
    "        A *= 2.0^-s\n",
    "        E *= 2.0^-s\n",
    "        U, V, Lu, Lv = _diff_pade13(A, E, ident)\n",
    "    end\n",
    "    \n",
    "    # factor once and solve twice\n",
    "    lu_piv = lu(-U + V)\n",
    "    R = lu_piv \\ (U + V)\n",
    "    L = lu_piv \\ (Lu + Lv + ((Lu - Lv) * R))\n",
    "    \n",
    "    # repeated squaring\n",
    "    for k in 1:s\n",
    "        L = R * L + L * R\n",
    "        R= R * R\n",
    "    end\n",
    "    return L[edges]\n",
    "end\n",
    "\n",
    "frechet_algo(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking <a name=\"benchmarking\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  466.981 ms (63915 allocations: 360.10 MiB)\n",
      "  80.813 ms (7379 allocations: 57.57 MiB)\n",
      "  1.729 s (5909 allocations: 311.93 MiB)\n",
      "  1.502 ms (20 allocations: 315.22 KiB)\n",
      "  1.810 ms (24 allocations: 609.55 KiB)\n",
      "  390.625 μs (211 allocations: 1.78 MiB)\n"
     ]
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "f = (W, _) -> sum(exp(W))\n",
    "\n",
    "@btime tangent_approx(f, $W_bench, $edges_bench, 0.01) \n",
    "@btime finite_diff(f, $W_bench, $edges_bench, 0.1) \n",
    "@btime forward_diff_j($W_bench, $edges_bench) \n",
    "@btime forward_diff_jvp($W_bench, $edges_bench) \n",
    "@btime frechet_block_enlarge($W_bench, $edges_bench) \n",
    "@btime frechet_algo($W_bench, $edges_bench);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
