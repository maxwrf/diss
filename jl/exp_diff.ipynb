{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation of objective function for weighted GNMs\n",
    "From: https://www.biorxiv.org/content/10.1101/2023.06.23.546237v1?med=mas.\n",
    "\n",
    "This is mostly concerned with finding the fastest way to compute the derivative of the matrix exponential.\n",
    "$$\n",
    "f(w_{i,j}) = (c_{i,k} * d_{i,j})^{\\omega}\n",
    "$$\n",
    "Where, the communicability matrix is defined as follows. This captures the proportion of signals, that propagate randomly from node i would reach node j over an infinite time horizon.\n",
    "$$\n",
    "c_{i,j} = e^{s^{-1/2}w_{i,j}s^{-1/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Polynomials\n",
    "using ForwardDiff\n",
    "using ExponentialAction\n",
    "using ExponentialUtilities\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       " 0.0  0.8  0.0\n",
       " 0.8  0.0  0.2\n",
       " 0.0  0.2  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = [0.0 0.8 0.0;\n",
    "     0.8 0.0 0.2; \n",
    "     0.0 0.2 0.0]\n",
    "demo_edge = [CartesianIndex(1, 2)]\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangent approximation\n",
    "Here we are computing the derivative for a single edge. We will use the tangent approximation to compute the derivative. \n",
    "This is the approach used in the Akarca paper.\n",
    "\n",
    "### Problem 1\n",
    "As far as I understand, there is a small but significant error in the implementation provided at https://github.com/DanAkarca/weighted_generative_models/blob/main/weighted_generative_model.m in line 197.\n",
    "When fitting the first-order polynomial, the independent variable should not be the range of the differences to the edge, which have been used, but should be the actual values, the edge is taking on.\n",
    "\n",
    "### Problem 2\n",
    "In the same implementation the derivative is approximated from the tangent approximation after nudging the edge (i,j) as well as (j,i).\n",
    "In a sense we are computing the derivative with respect to both (i,j) and (j,i) at once.\n",
    "First of all, this will create computational overhead, as the derivatives (i.e., the jacobian) will also always be symmetric.\n",
    "Secondly, this will create a problem when we are trying to compute the gradient, which we currently simply take as the the derivative.\n",
    "But then we update both elements in the W matrix with the same gradient which is the derivative with respect to nudging both (i,j) and (j,i) - thus our update to the objective function is overshooting.\n",
    "\n",
    "See line 208 and 208."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.038767237882607476], [0.7456781939183535])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function paper_tangent_approx(W, edges::Vector{CartesianIndex{2}}, resolution = 0.01, min = -0.25, max = 0.25)\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(min:resolution:max)\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)\n",
    "        \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        # Line 197 in MATLAB code\n",
    "        x = 1:length(reps)\n",
    "        results[i_edge] = fit(x, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "paper_tangent_approx(W, demo_edge, 0.01), paper_tangent_approx(W, demo_edge, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct tangent approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.8459047353259335], [4.851647952441173])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tangent_approx(W, edges::Vector{CartesianIndex{2}}, \n",
    "    resolution = 0.01, min = -0.25, max = 0.25)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    rep_vec = collect(min:resolution:max)\n",
    "\n",
    "    for (i_edge, edge_idx) in enumerate(edges)     \n",
    "        # Points for evaluation\n",
    "        edge_val = W[edge_idx]\n",
    "        reps = [edge_val * (1 + i) for i in rep_vec]\n",
    "\n",
    "        # For each nudge save difference in communicability \n",
    "        sum_comm = zeros(length(reps))\n",
    "        for (i_rep, rep) in enumerate(reps)\n",
    "            W_copy = copy(W)\n",
    "            W_copy[edge_idx] = W_copy[edge_idx[2], edge_idx[1]] = rep\n",
    "            comm = exp(W_copy)\n",
    "            sum_comm[i_rep] = sum(comm)\n",
    "        end\n",
    "\n",
    "        results[i_edge] = fit(reps, sum_comm, 1)[1]\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "tangent_approx(W, demo_edge, 0.01), tangent_approx(W, demo_edge, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference method\n",
    "Compute the approximate derivative using the finite difference method.\n",
    "https://en.wikipedia.org/wiki/Finite_difference_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.826503769214696], [4.827101432180703])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function finite_diff(W, edges::Vector{CartesianIndex{2}}, delta::Float64)::Vector{Float64}\n",
    "    results = zeros(length(edges))\n",
    "    for (i_edge, edge_idx) in enumerate(edges) \n",
    "        # Evaluate the function at two nearby points\n",
    "        W_copy = copy(W)\n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] + delta\n",
    "        f_plus_delta = sum(exp(W_copy))\n",
    "        \n",
    "        W_copy[edge_idx]  = W_copy[edge_idx] - 2*delta\n",
    "        f_minus_delta = sum(exp(W_copy))\n",
    "\n",
    "        # Calculate the derivative approximation\n",
    "        results[i_edge] = (f_plus_delta - f_minus_delta) / (2 * delta)\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n",
    "\n",
    "\n",
    "finite_diff(W, demo_edge, 0.01)*2, finite_diff(W, demo_edge,0.2)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation\n",
    "Results are derived in the numerator layout, which means that the dimensionality of the input is added to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.8612290825962985"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_j(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    # Column indices for retrieval\n",
    "    indices = collect(CartesianIndices(W))\n",
    "    index_vec = sort(vec(indices), by = x -> x[1])\n",
    "\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    J = ForwardDiff.jacobian(diff_exp, W)\n",
    "\n",
    "    results = zeros(length(edges))\n",
    "    tangent = vec(permutedims(exp(W), [2, 1]))\n",
    "    for (i_edge, edge) in enumerate(edges)\n",
    "        # we get all partial derivative positions that are non-zero\n",
    "        Jₓ = J[:, findfirst(x -> x == edge, index_vec)]\n",
    "        results[i_edge] = dot(Jₓ, tangent)\n",
    "    end\n",
    "\n",
    "    return results .* 2\n",
    "end\n",
    "\n",
    "forward_diff_j(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Differentiation with Jacobian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 4.861229082596298"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward_diff_jvp(W::Matrix{Float64}, edges::Vector{CartesianIndex{2}})::Vector{Float64}\n",
    "    tangent = exp(W)\n",
    "    diff_exp(W) = exponential!(copyto!(similar(W), W), ExpMethodGeneric())\n",
    "    g(t) = diff_exp(W + t * tangent)\n",
    "    JVP = ForwardDiff.derivative(g, 0.0)\n",
    "    return JVP[edges] .* 2\n",
    "end\n",
    "\n",
    "\n",
    "forward_diff_jvp(W, demo_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.608 s (279080 allocations: 1.83 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88.504 ms (7743 allocations: 60.52 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.728 s (5961 allocations: 312.16 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.700 ms (39 allocations: 432.30 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "241-element Vector{Float64}:\n",
       "  9.435189136678718\n",
       "  5.195743270503177\n",
       " 13.467930447408666\n",
       "  4.729657826331669\n",
       " 37.97108049898049\n",
       " 35.63246316466708\n",
       " 40.14988627804099\n",
       " 52.20936212065478\n",
       " 61.064680835548295\n",
       " 22.35336492281464\n",
       "  ⋮\n",
       " 14.609734107238188\n",
       " 17.08779448253476\n",
       " 34.43686560573628\n",
       " 11.032408188686084\n",
       " 22.008071742700885\n",
       " 25.426604501682263\n",
       "  6.7603142647465155\n",
       "  7.540743730991197\n",
       " 20.12146474452763"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function init_sparse_matrix(n = 100, density = 0.2)\n",
    "    # Initialize a sparse matrix\n",
    "    W = zeros(n, n)\n",
    "    for i in 1:n, j in 1:n\n",
    "        if rand() < (density / 2)\n",
    "            W[i, j] = W[j,i] = rand()\n",
    "        end\n",
    "    end\n",
    "    return W\n",
    "end\n",
    "\n",
    "W_bench = init_sparse_matrix(50, 0.10)\n",
    "edges_bench = findall(x -> x != 0, W_bench)\n",
    "\n",
    "@btime tangent_approx($W_bench, $edges_bench, 0.01)\n",
    "@btime finite_diff($W_bench, $edges_bench, 0.1)\n",
    "@btime forward_diff_j($W_bench, $edges_bench)\n",
    "@btime forward_diff_jvp($W_bench, $edges_bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `cartesian_indices` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `cartesian_indices` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/code/diss/jl/exp_diff.ipynb:1"
     ]
    }
   ],
   "source": [
    "all_indices = [[[] for _ in 1:length(cartesian_indices)] for _ in 1:length(cartesian_indices)]\n",
    "for i in 1:size(A, 1)^2\n",
    "    for j in 1:size(A, 1)^2\n",
    "        push!(all_indices[i][j], index_vec_r[i], index_vec_c[j])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Any combination of edges that includes a self-loop needs to be removed later\n",
    "cycle_indices = CartesianIndex[]\n",
    "for i in 1:size(A, 1)^2\n",
    "    for j in 1:size(A, 1)^2\n",
    "        if ((all_indices[i][j][1][1] == all_indices[i][j][1][2])||\n",
    "            (all_indices[i][j][2][1] == all_indices[i][j][2][2])||\n",
    "            (all_indices[i][j][1] == all_indices[i][j][2]) ||\n",
    "            ((all_indices[i][j][1][1] == all_indices[i][j][2][2]) && \n",
    "             (all_indices[i][j][1][2] == all_indices[i][j][2][1]))\n",
    "            )\n",
    "            push!(cycle_indices, CartesianIndex(i, j))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
