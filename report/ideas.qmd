---
title: "Dissertation: Background and Literature"
subtitle: "Author: mw894"
format:
  pdf:
    documentclass: article
    highlight-style: atom-one
    numbersections: true
    fontsize: 10pt
    toc: true
    colorlinks: true
    geometry:
      - top=30mm
      - left=20mm
      - right=20mm
bibliography: ref.bib
jupyter: python3
---

# Brain Formation and structure
## Parsimonious (wiring) principles
These reach back very far to as Ramon y Cajal (1899).

### Network wiring cost
There is overvelwheling evidence in favor of this hypothesis [@bullmore_economy_2012]:
\begin{itemize}
    \item Cost of building and maintaining axonal connections and speed of transmission increase in wiring volume which is proportional to length (D)
    \item White matter grows faster than grey-matter as function of brain size, driven by increase in axonal diameter and number synapses per neuron
    \item Fraction of grey matter neurons that send myelinated axons into white matter slowly reduces with brain size
    \item The probability distribution of connection distances is skewed towards short distances that will be relatively parsimonious in wiring cost
\end{itemize}
Factors which potentially explain the deviations from this simple wiring principle [@bullmore_economy_2012]:
\begin{itemize}
    \item Volume exclusions = Due to the limited size of the brain, axonal projections must perturbate from the straight line connection
    \item Functional properties = Example: Monosynaptic vs. Polysynaptic nerves, when you need low latency you would build a high cost long axonal connection
\end{itemize}

### Network running cost
Not part of Cajal's initial principal's, but metabolic cost of running the brain has become an increasingly important principle for network formation.
\begin{itemize}
    \item Bigger brains are metabolically more expensive, with a rate increasing faster than overall body oxygen increase as function of weight
    \item The cost attributable to maintenance of electrochemical gradients across membranes
    \item Cost increases in overall neuronal membrane size as well as axonal length and diameter, this controlling length also limits energy requirements
    \item Limitation: Networks often functionally configured to be less expensive than possible within the anatomical constraints (network formation), e.g., through sparse coding
\end{itemize}

## Network topology - Small-world architecture
\begin{itemize}
    \item Watts and Strogatz analyses nervous system of C. Elegans as binary graph and found both high clustering and short path-length
    \item Brains posses the small-world properties of high clustering and global efficiency, a modular community structure and a heavy tailed degree distribution, that indicate high connected nodes or hubs
    \item High clustering and efficiency is attractive to allow for segregation of process (e.g., visual analysis) and integration (distributed process, executive functions)
    \item IQ negatively correlated with paht lengths, which supports workspace theory, that effortful tasks depend on oscillations in ensembles of anatomically distributed regions
    \item Topological modularity might also be helpful to evolution, making it more robust to rewiring
\end{itemize}

## Combining Parsimonious principle and Topological characteristics
This is to investigate how the physical embedding is constraint or constraints network topologies. How do the two concepts relate.
\begin{itemize}
    \item Molecular gradients of attractive and repulsive guidance cues determine the trajectories of growing nerve fibres
    \item The spatial distribution of molecules that are involved neural development and physical constraints limit range of many connections, which could result in a bias towards high clustering
    \item Topological characteristics that favour segregation are also parsimonious in wiring cost
    \item Long distance connections preferentially link to hub regions, and they arew costly but reduce path length for information transfer
    \item Evidence: In C. Elegans, wiring cost is not strictly minimized but the networks show similarities to networks that achieve cost constrained spatial embeddings of topologically complex networks
    \item Issue in MEG networks: Usually stationary picture of networks, but when studied under stimuli, MEG networks with high efficiency emerge under high effort tasks and vice versa
\end{itemize}

# Generative network models
These models were first developed and applied to connectome data by @betzel_generative_2016.
Can be considered as an extensions to the economic trade-off principle, which states that configuration of the brain can be accounted for by economic balancing of wiring cost minimization and topological efficiency maximization.

## Evaluation
$$
E = max(KS_{k}, KS_{c}, KS_{b}, KS_{e})
$$
to quantify the difference between the synthetic and observed data using the Kolmogorov-Smirnov statistic.
The corresponding statistic is computed for every vertex, and then the distributions are compared.
\begin{itemize}
    \item $KS_{K}$ Nodal degree = Number of edges that are incident to a vertex
    \item $KS_{C}$ Clustering (coefficient) = Measure of the degree to which nodes in a graph tend to cluster together, proportion of possible connections realized among the neighbors of a vertex (Where the neighbor of a vertex are all the other connected vertices)
    \item $KS_{b}$ Betweenness centrality = Measures the centrality of a vertex by investigating the number of shortest paths that pass through the vertex (For every pair of vertices there exists a shortest path between them minimizing either the number of edges or the summed weights of the edges)
    \item $KS_{e}$ Edge length = Sum off al the edge length that are incident to a vertex 
\end{itemize}

## Optimization

## Rules
### Geometric
\begin{itemize}
    \item Promotion of low cost connections is promoted, but forming only the shortest connections, produces lack of long distance connections, which increases path length, and reduces efficiency
    \item Problems in reproducing clustering and edge length distributions simultaneously ($KS_{c}, KS_{e}$), this is because strong distance penalty required to make high clustering but then lacking long distance connections 
\end{itemize}
### Degree
### Clustering
### Homophilic
Prioritize wiring of nodes with with overlapping connectivity patterns.
These principles are often interpreted as manifestation of Hebbian learning, when two unconnected nodes share multiple neighbors, they will likely showcase similar firing behavior.
Consequently, they are also more likely to connect with each other in the future.

#### Neighbors
For any combination of nodes get the number of common neighbors 
$$
\sum_{l}{A_{il} A_{jl}}
$$ {#eq-neigbors}

#### Matching index
Intersection of i's and j's neighbors except each other over the union of i's and j's neighbors. 
$$
\frac{|N_{i/j} \cap N_{j/i}|}{|N_{i/j} \cup N_{j/i}|}
$$ {#eq-matching-index}
\begin{itemize}
    \item Normalized measure of the overlap in two vertexes neighborhoods
    \item that eta and gamma seem to trade offer with each other, such that a connectome is either shaped by geometry or non-geometric constraints
\end{itemize}

# Computational developmental models
Computational developmental models can generally be divided into those focusing on **cognitive development**, that is they try to perform tasks inspired by neurobiological principles, vs. those that focus on **neurobiological development**, which are models that try to capture neurobiological development.
Cognitive models are fit to cognitive data with learning inspired by neurobiology, thereby trying to capture trajectories of change [@astle_toward_2023].
These include: 
\begin{itemize}
    \item Cascade correlation models
    \item Knowledge-based cascade correlations
    \item Neuroconstructivist representation learning  
    \item Spatially embedded recurrent neural networks
\end{itemize}
On the other hand, neurobiological development models are fit to neurobiolgical data and try to capture trajectories in physiological development.
The models include:
\begin{itemize}
    \item Generative network models
    \item Synaptic pruning in development 
    \item Growth cone chemotaxis 
\end{itemize}w

## Cascade correlation neural network (CC)
\begin{itemize}
    \item Classical input output networks are trained to solve a supervised problem
    \item At some point, the model will struggle to improve performance given the limited computational capacity (restricted number of neurons / layers)
    \item A new unit is then added as hidden layers (?) which has been trained independently as to maximize the correlation with the error (a pool of candidates are trained)
    \item This arguably reassembles neurogenesis as it can solve a particular computation which could not have been solved previously (this is done iteratively)
    \item Different to regular neuronal networks, the architecture of this network does not require an a priori guess
    \item Criticism: Will fail to use prior knowledge to learn because they do not have any
\end{itemize}

## Knowledge based cascade correlations (KNCC)
\begin{itemize}
    \item An extension to the CCs, models that can recruit an entire sub-network based on a previously learned task
\end{itemize}

## Spatially embedded recurrent neural networks (seRNNs)
\begin{itemize}
    \item One of the only models, that merges biophysical principles with task-solving
    \item Exist in a 3D space, where connection weights are penalized as a function of their lengths
    \item seRNNS are limited in their communication via direct and indirect connections
    \item These constraints are placed into a single optimization problem
    \item The networks embody modularity and small-worldness, both of which are crucial development principles
    \item As they are trained, neurons within the seRNNs become increasingly specialized to distinct aspects, so we can observed a functional specialization that is spatially patterned 
\end{itemize}


# Graphs
## Rich clubs
\begin{itemize}
    \item Measure the extent by which well connected nodes are connected to each other
    \item Networks of high rich-club coefficients have many connections between nodes of high degree
\end{itemize}

# Automatic differentiation 
\begin{itemize}
\end{itemize}

# Random dictionary
\begin{itemize}
    \item Gray matter = Composed of the neurons (appears darker due ot higher levels of melanin)
    \item White matter = Mainly made up of myelinated axons also called tracts (light appearance through the lipid content of myelin)
    \item Monosynaptic reflex = There is only one synapse between the afferent nerve and the efferent nerve (slow latency)
    \item Polysnaptic reflex = There are more than one synapse between the nerves, such that there is higher latency
    \item Sparse coding = Neural coding that represents the information by activation of small subset of the available neurons
    \item Small-world = Combine random and regular topological properties, i.e., high efficiency (short path length) and high clustering
    \item Community structure = Sub-global organization of a complex network, an example is modular organization
    \item Heavy-tailed degree distributions = Proportion of high degree nodes is larger than in random graphs (so that they can be hubs)
    \item Computational neuroconstructivism = Computationally formalize neuroconstructivist principles by parameterized interactions between populations over time
\end{itemize}

# References